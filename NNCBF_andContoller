import torch
from torch import nn
import numpy as np


class CBF(nn.Module):

    def __init__(self, state_dim, obstacle_dim, control_dim, preprocess=None):
        super().__init__()
        self.state_dim = state_dim
        self.obstacle_dim = obstacle_dim
        self.control_dim = control_dim
        self.preprocess = preprocess

        self.conv_layer0 = nn.Conv1d(state_dim, 64, 1)
        self.conv_layer1 = nn.Conv1d(64, 128, 1)
        self.conv_layer2 = nn.Conv1d(128, 128, 1)
        self.conv_layer3 = nn.Conv1d(128, 128, 1)
        self.conv_layer4 = nn.Conv1d(128, 1, 1)
        self.activation_func = nn.ReLU()
        self.output_func = nn.Tanh()

    def forward(self, state, obstacle):
        """
        args:
            state (bs, state_dim)
            obstacle (bs, obstacle_dim, state_dim)
        returns:
            h (bs, obstacle_dim)
        """
        state = torch.unsqueeze(state, 2)    # (bs, state_dim, 1)
        obstacle = obstacle.permute(0, 2, 1) # (bs, state_dim, obstacle_dim)
        state_diff = state - obstacle

        if self.preprocess is not None:
            state_diff = self.preprocess(state_diff)

        x = self.activation_func(self.conv_layer0(state_diff))
        x = self.activation_func(self.conv_layer1(x))
        x = self.activation_func(self.conv_layer2(x))   # (bs, 128, obstacle_dim)
        x = self.activation_func(self.conv_layer3(x))
        x = self.conv_layer4(x)
        h = torch.squeeze(x, dim=1)          # (bs, obstacle_dim)
        return h


class NNController(nn.Module):

    def __init__(self, state_dim, obstacle_dim, control_dim, preprocess=None, output_scale=1.0):
        super().__init__()
        self.state_dim = state_dim
        self.obstacle_dim = obstacle_dim
        self.control_dim = control_dim
        self.preprocess = preprocess

        self.conv_layer0 = nn.Conv1d(state_dim, 64, 1)
        self.conv_layer1 = nn.Conv1d(64, 128, 1)
        self.conv_layer2 = nn.Conv1d(128, 128, 1)
        self.fc_layer0 = nn.Linear(128 + control_dim + state_dim, 128)
        self.fc_layer1 = nn.Linear(128, 64)
        self.fc_layer2 = nn.Linear(64, control_dim)
        self.activation_func = nn.ReLU()
        self.output_func = nn.Tanh()
        self.output_scale = output_scale

    def forward(self, state, obstacle, nominal_control, state_error):
        """
        args:
            state (bs, state_dim)
            obstacle (bs, obstacle_dim, state_dim)
            nominal_control (bs, control_dim)
            state_error (bs, state_dim)
        returns:
            control (bs, control_dim)
        """
        state = torch.unsqueeze(state, 2)    # (bs, state_dim, 1)
        obstacle = obstacle.permute(0, 2, 1) # (bs, state_dim, obstacle_dim)
        state_diff = state - obstacle

        if self.preprocess is not None:
            state_diff = self.preprocess(state_diff)
            state_error = self.preprocess(state_error)

        x = self.activation_func(self.conv_layer0(state_diff))
        x = self.activation_func(self.conv_layer1(x))
        x = self.activation_func(self.conv_layer2(x))   # (bs, 128, obstacle_dim)
        x, _ = torch.max(x, dim=2)              # (bs, 128)
        x = torch.cat([x, nominal_control, state_error], dim=1) # (bs, 128 + control_dim)
        x = self.activation_func(self.fc_layer0(x))
        x = self.activation_func(self.fc_layer1(x))
        x = self.output_func(self.fc_layer2(x)) * self.output_scale
        control = x + nominal_control
        return control


class ControlAffineDynamics(nn.Module):

    def __init__(self, state_dim, control_dim, preprocess=None, extended_state_dim=0):
        super().__init__()
        self.state_dim = state_dim
        self.control_dim = control_dim
        self.preprocess = preprocess
        self.extended_state_dim = extended_state_dim

        self.f_fc_layer0 = nn.Linear(state_dim + extended_state_dim, 64)
        self.f_fc_layer1 = nn.Linear(64, 128)
        self.f_fc_layer2 = nn.Linear(128, state_dim)

        self.B_fc_layer0 = nn.Linear(state_dim + extended_state_dim, 64)
        self.B_fc_layer1 = nn.Linear(64, 128)
        self.B_fc_layer2 = nn.Linear(128, state_dim * control_dim)

        self.activation_func = nn.Tanh()

    def forward(self, state, control):
        """ Compute f(s) and B(s) such that sdot = f(s) + B(s)u.
        args:
            state (bs, state_dim)
            control (bs, control_dim)
        returns:
            f (bs, state_dim)
            B (bs, state_dim, control_dim)
        """
        if self.preprocess is not None:
            state = self.preprocess(state) # (bs, state_dim + extended_state_dim)

        x = self.activation_func(self.f_fc_layer0(state))
        x = self.activation_func(self.f_fc_layer1(x))
        f = self.f_fc_layer2(x)

        x = self.activation_func(self.B_fc_layer0(state))
        x = self.activation_func(self.B_fc_layer1(x))
        B = self.B_fc_layer2(x).view(-1, self.state_dim, self.control_dim)

        return f, B
